{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 32 baseline results.\n",
      "We have 224 experimental results.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "baselines: Path = Path(\"/home/kisamefishfry/paperResults/clangaxtls.baseline.dedupe.json\")\n",
    "experimental_results: Path = Path(\"/home/kisamefishfry/paperResults/clangaxtls.dedupe.json\")\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "\n",
    "with open(experimental_results) as f:\n",
    "    experimental_results = json.load(f)\n",
    "\n",
    "lonely_baselines = copy.deepcopy(baselines)\n",
    "lonely_experimental_results = copy.deepcopy(experimental_results)\n",
    "\n",
    "class IntRange:\n",
    "    def __init__(self, lower_bound_inclusive, upper_bound_exclusive):\n",
    "        self.lower_bound_inclusive = lower_bound_inclusive\n",
    "        self.upper_bound_exclusive = upper_bound_exclusive\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return isinstance(item, int) and (self.lower_bound_inclusive <= item < self.upper_bound_exclusive)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"IntRange({self.lower_bound_inclusive, self.upper_bound_exclusive})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"[{self.lower_bound_inclusive}:{self.upper_bound_exclusive})\"\n",
    "\n",
    "    def to_json(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "for e in experimental_results:\n",
    "    toks = e['original_line'].split(':')\n",
    "    try:\n",
    "        e['original_line'] = IntRange(int(toks[0]), int(toks[1]) + 1)\n",
    "    except Exception as ex:\n",
    "        e['original_line'] = []\n",
    "    #print('\\t'.join([\"experimental\", *[str(s) for s in e.values()]]).replace(\"\\n\", \"\"))\n",
    "\n",
    "    if e['function_line_range'] == 'ERROR':\n",
    "        e['function_line_range'] = []\n",
    "    else:\n",
    "        toks = e['function_line_range'].split(':')\n",
    "        try:\n",
    "            e['function_line_range'] = IntRange(int(toks[1]), int(toks[2]) + 1)\n",
    "        except Exception as ex:\n",
    "            logging.exception(f\"e was {e}\")\n",
    "    e['presence_condition'] = str(e['presence_condition'])\n",
    "\n",
    "print(f\"We have {len(baselines)} baseline results.\")\n",
    "print(f\"We have {len(experimental_results)} experimental results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 176.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def check_config(configuration,condition):\n",
    "    baseline_var_mapping = {}\n",
    "    for var in configuration:\n",
    "        if var.startswith('DEF'):\n",
    "            if '=' in var:\n",
    "                baseline_var_mapping[re.sub(r\"^DEF(_.*)=.*\", r\"USE\\1\", var)] = \n",
    "            else:\n",
    "                baseline_var_mapping[re.sub(r\"^(DEF_.*)\", r\"\\1\", var)] = True\n",
    "        elif var.startswith('UNDEF'):\n",
    "            baseline_var_mapping[re.sub(r\"^UN(DEF_.*)\", r\"\\1\", var)] = False\n",
    "        else:\n",
    "            raise RuntimeError(f\"Don't know how to handle variable {var}\")\n",
    "\n",
    "    s = Solver()\n",
    "    for var, val in baseline_var_mapping.items():\n",
    "        var = Bool(var)\n",
    "        if val:\n",
    "            s.add(var)\n",
    "        else:\n",
    "            s.add(Not(var))\n",
    "\n",
    "    for mat in re.findall(\"DEF_[a-zA-Z0-9_]+\", condition):\n",
    "        exec(f\"{mat} = Bool('{mat}')\")\n",
    "           \n",
    "    for mat in re.findall(\"USE_[a-zA-Z0-9_]+\", condition):\n",
    "        exec(f\"{mat} = Int('{mat}')\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            s.add(eval(condition))  # TODO Definitely need to do more transformation here.\n",
    "            break\n",
    "        except NameError as ne:\n",
    "            print('failure')\n",
    "            var = re.search(\"name '(.*)' is not defined\", str(ne))\n",
    "            exec(f\"{var.group(1)} = Int('{var.group(1)}')\")\n",
    "    return s.check() == sat\n",
    "\n",
    "def match_stats(baseline_result: dict, experimental_result: dict) -> Tuple:\n",
    "    \"\"\"\n",
    "    Returns a vector of different match information.\n",
    "    (a, b, c)\n",
    "    a = True iff baseline and experimental have the same line number, message, and file.\n",
    "    b = True iff baseline and experimental have the same message, file, and baseline is within experimental's function scope.\n",
    "    c = True iff baseline's configuration is compatible with experimental's presence condition.\n",
    "    \"\"\"\n",
    "\n",
    "    a = (baseline_result['sanitized_message'] == experimental_result['sanitized_message'] and \\\n",
    "         baseline_result['input_line'] in experimental_result['original_line'] and\\\n",
    "         baseline_result['input_file'].split('.')[0] == experimental_result['input_file'].split('.')[0])\n",
    "\n",
    "    b = (baseline_result['sanitized_message'] == experimental_result['sanitized_message'] and \\\n",
    "         baseline_result['input_line'] in experimental_result['function_line_range'] and\\\n",
    "         baseline_result['input_file'].split('.')[0] == experimental_result['input_file'].split('.')[0])\n",
    "\n",
    "    c = False    \n",
    "    \n",
    "    if experimental_result['feasible'] and 'Or(None' not in experimental_result['presence_condition'] and experimental_result['presence_condition'] not in ['Or(None)', 'None'] and (a or b):  # Don't bother doing this expensive step when the file and line number are different.\n",
    "        if isinstance(baseline_result['configuration'],list):\n",
    "            for config in baseline_result['configuration']:\n",
    "                currC = check_config(config,experimental_result['presence_condition'])\n",
    "                c = c or currC\n",
    "                if c:\n",
    "                    break\n",
    "        else:\n",
    "            c = check_config(baseline_result['configuration'],experimental_result['presence_condition'])\n",
    "        \n",
    "    return a, b, c\n",
    "\n",
    "def tupleize(func, args): return func(*args), tuple(args)\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Note that results depend on the order of keys in this dictionary, because once we find a match_stats for one level we do not keep searching for the next.\n",
    "#  E.g., for a given report, we will first look for results with which it has a (True, True, True) report. If it has one, we do not continue searching for\n",
    "#  matches for (False, True, True), (True, False, True), etc.\n",
    "result_hierarchy = {(True, True, True): 0, (False, True, True): 0, (True, False, True): 0, (True, True, False): 0, (False, True, False): 0, (False, False, True): 0, (True, False, False): 0, (False, False, False): 0}\n",
    "\n",
    "report = []\n",
    "for b in tqdm.tqdm(baselines):\n",
    "    # Results are (baseline, desugared, match tuple)\n",
    "    results = [(b, e, match_stats(b, e)) for e in experimental_results]\n",
    "    found = False\n",
    "    for r in result_hierarchy.keys():\n",
    "        for res in results:\n",
    "            if res[2] == r:\n",
    "                found = True\n",
    "                result_hierarchy[r] += 1\n",
    "                # -----\n",
    "                # Here is where you compile information about any specific reports you need. This block of code\n",
    "                # iterates through all baselines and finds the highest level of matching that is available.\n",
    "                # So, for example, if you wanted to collect all of the unmatched originals, you would uncomment out this line of code:\n",
    "                #\n",
    "                if (r != (True, True, True) and r != (False, True, True)):\n",
    "                    report.append(res[0])\n",
    "                break # DO NOT DELETE THE BREAK!\n",
    "        if found:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Number of baseline results: 32\n",
      "Number of desugared results: 224\n",
      "Number of feasible desugared results: 109\n",
      "Number of exact matches: 8\n",
      "Number of partial matches: 1\n",
      "Number of unmatched: 23\n"
     ]
    }
   ],
   "source": [
    "# Change this if you want the reports printed out differently. For example, if above you collected unmatched reports,\n",
    "#  then here, you probably want something like\n",
    "#\n",
    "#print(json.dumps(report, indent=2))\n",
    "#\n",
    "#_4488 = dict()\n",
    "#for e in report:\n",
    "#    key = (e['message'], e['input_file'], e['input_line'])\n",
    "#    if key not in _4488:\n",
    "#        _4488[key] = 0\n",
    "#    _4488[key] += 1\n",
    "#print(len(_4488))\n",
    "print('-----------')\n",
    "print(f\"Number of baseline results: {len(baselines)}\")\n",
    "print(f\"Number of desugared results: {len(experimental_results)}\")\n",
    "print(f\"Number of feasible desugared results: {len(list(filter(lambda e: e['feasible'],experimental_results)))}\")\n",
    "print(f\"Number of exact matches: {result_hierarchy[(True, True, True)]}\")\n",
    "print(f\"Number of partial matches: {result_hierarchy[False, True, True]}\")\n",
    "print(f\"Number of unmatched: {sum(v for k, v in result_hierarchy.items() if k not in [(True, True, True), (False, True, True)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configurations with warnings: 6\n",
      "Average warnings per config is 0.032\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for b in baselines:\n",
    "    conf = str(b.get('configuration'))\n",
    "    if conf not in results:\n",
    "        results[conf] = 0\n",
    "    results[conf] += 1\n",
    "\n",
    "print(f\"Number of configurations with warnings: {len(results)}\")\n",
    "import random\n",
    "while len(results.keys()) < 1000:\n",
    "    results[''.join(random.choices(string.ascii_letters, k=5))] = 0\n",
    "\n",
    "print(f\"Average warnings per config is {float(sum(results.values()))/float(len(results.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 224/224 [00:00<00:00, 1490.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Number of desugared results: 224\n",
      "Number of baseline results: 32\n",
      "Number of exact matches: 9\n",
      "Number of partial matches: 1\n",
      "Number of unmatched: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_hierarchy = {(True, True, True): 0, (False, True, True): 0, (True, False, True): 0, (True, True, False): 0, (False, True, False): 0, (False, False, True): 0, (True, False, False): 0, (False, False, False): 0}\n",
    "\n",
    "for e in tqdm.tqdm(experimental_results):\n",
    "    results = [(b, e, match_stats(b, e)) for b in baselines]\n",
    "    found = False\n",
    "    for r in result_hierarchy.keys():\n",
    "        for res in results:\n",
    "            if res[2] == r:\n",
    "                found = True\n",
    "                result_hierarchy[r] += 1\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "print('-----------')\n",
    "print(f\"Number of desugared results: {len(experimental_results)}\")\n",
    "print(f\"Number of baseline results: {len(baselines)}\")\n",
    "print(f\"Number of exact matches: {result_hierarchy[(True, True, True)]}\")\n",
    "print(f\"Number of partial matches: {result_hierarchy[False, True, True]}\")\n",
    "print(f\"Number of unmatched: {sum(v for k, v in result_hierarchy.items() if k not in [(True, True, True), (False, True, True)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(set([(e['message'], e['input_file'], e['input_line']) for e in baselines])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poor man's stop full execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxxxxxxxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mxxxxxxxxx\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xxxxxxxxx' is not defined"
     ]
    }
   ],
   "source": [
    "print(xxxxxxxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate Varbugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "baselines: Path = Path(\"C:/Users/kisam/bp.json\")\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "    \n",
    "results = dict()\n",
    "for e in baselines:\n",
    "    key = (e['message'], e['input_file'], e['input_line'])\n",
    "    if key not in results:\n",
    "        results[key] = []\n",
    "    results[key].append(e)\n",
    "\n",
    "def subset(a,b):\n",
    "    al = a['configuration']\n",
    "    bl = b['configuration']\n",
    "    if len(a) == len(b):\n",
    "        return (False,None)\n",
    "    smaller = al if len(al) < len(bl) else bl\n",
    "    bigger = al if len(al) > len(bl) else bl\n",
    "    allIn = True\n",
    "    for x in smaller:\n",
    "        if x not in bigger:\n",
    "            allIn = False\n",
    "            break\n",
    "    if allIn:\n",
    "        aa = copy.deepcopy(a)\n",
    "        aa['configuration'] = smaller\n",
    "        return (True, aa)\n",
    "    return (False, None)\n",
    "\n",
    "def common(a,b):\n",
    "    al = a['configuration']\n",
    "    bl = b['configuration']\n",
    "    if len(a) != len(b):\n",
    "        return (False,None)\n",
    "    swapIn = []\n",
    "    for x in al:\n",
    "        if x not in bl:\n",
    "            swap = x[2:] if x.startswith('UNDEF') else 'UN' + x\n",
    "            if swap not in bl:\n",
    "                return (False, None)\n",
    "        else:\n",
    "            swapIn.append(x)\n",
    "    if len(swapIn) == len(al) -1:\n",
    "        aa = copy.deepcopy(a)\n",
    "        aa['configuration'] = swapIn\n",
    "        return (True, aa)\n",
    "    return (False, None)\n",
    "\n",
    "def eq(a,b):\n",
    "    if len(a) != len(b):\n",
    "        return False\n",
    "    for x in a['configuration']:\n",
    "        if x not in b['configuration']:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "dedupe = []\n",
    "for k,v in results.items():\n",
    "    #v = list of all same bug\n",
    "    anyMatch = True\n",
    "    while anyMatch:\n",
    "        anyMatch = False\n",
    "        newList = []\n",
    "        i = 0\n",
    "        while i < len(v):\n",
    "            matched = False\n",
    "            j = i + 1\n",
    "            while j < len(v):\n",
    "                res = subset(v[i],v[j])\n",
    "                if res[0]:\n",
    "                    newList.append(res[1])\n",
    "                    matched = True\n",
    "                    anyMatch = True\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                res = common(v[i],v[j])\n",
    "                if res[0]:\n",
    "                    newList.append(res[1])\n",
    "                    matched = True\n",
    "                    anyMatch = True\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                if eq(v[i],v[j]):\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                j += 1\n",
    "            if not matched:\n",
    "                newList.append(v[i])\n",
    "            i += 1\n",
    "        v = newList\n",
    "    dedupe.extend(newList)\n",
    "    \n",
    "with open(\"C:/Users/kisam/dbp.json\", 'w') as f:\n",
    "    json.dump(dedupe, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate Real World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "baselines: Path = Path(\"/home/kisamefishfry/paperResults/clangaxtls.baseline.json\")\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "    \n",
    "results = dict()\n",
    "for e in baselines:\n",
    "    key = (e['message'], e['input_file'], e['input_line'])\n",
    "    if key not in results:\n",
    "        results[key] = []\n",
    "    results[key].append(e)\n",
    "\n",
    "for k,v in results.items():\n",
    "    v[0]['configuration'] = [v[0]['configuration']]\n",
    "    for x in v[1:]:\n",
    "        v[0]['configuration'].append(x['configuration'])\n",
    "    \n",
    "with open(\"/home/kisamefishfry/paperResults/clangaxtls.baseline.dedupe.json\", 'w') as f:\n",
    "    lst = [x[0] for x in results.values()]\n",
    "    print(len(lst))\n",
    "    json.dump(lst, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate Desugared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "base: Path = Path(\"/home/kisamefishfry/paperResults/clangaxtls.json\")\n",
    "\n",
    "with open(base) as f:\n",
    "    base = json.load(f)\n",
    "\n",
    "def EQ(a,b):\n",
    "    if a['original_line'] != 'ERROR':\n",
    "        return a['sanitized_message'] == b['sanitized_message'] and a['input_file'] == b['input_file'] and a['original_line'] == b['original_line'] and a['feasible'] == b['feasible']\n",
    "    a1 = a['function_line_range'].split(':')[-1]\n",
    "    a2 = a['function_line_range'].split(':')[-1]\n",
    "    b1 = b['function_line_range'].split(':')[-2]\n",
    "    b2 = b['function_line_range'].split(':')[-1]\n",
    "    return a1 == b1 and a2 == b2 and a['sanitized_message'] == b['sanitized_message'] and a['input_file'] == b['input_file'] and a['feasible'] == b['feasible']\n",
    "\n",
    "print(len(base))\n",
    "i = 0\n",
    "while i < len(base):\n",
    "    j = i+1\n",
    "    while j < len(base):\n",
    "        if EQ(base[i],base[j]):\n",
    "            base[i]['presence_condition'] = 'Or(' + base[i]['presence_condition'] + ',' + base[j]['presence_condition'] +')'\n",
    "            base.pop(j)\n",
    "        j += 1\n",
    "    i += 1\n",
    "print(i)\n",
    "with open('/home/kisamefishfry/paperResults/clangaxtls.dedupe.json','w') as x:\n",
    "    x.write(json.dumps(base,indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the notebook, we have a few structures.\n",
    "- summary: A dictionary mapping 3-tuples corresponding to results to a list of pairs of results.\n",
    "- lonely_baselines: A list of baseline results for which no matching experimental result was found.\n",
    "- lonely_experimental_results: A list of experimental results for which no matching baseline was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "\n",
    "This code randomly samples a result from each classification and prints it for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps({\"summary\": {str(k): len(summary[k]) for k in summary.keys()}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for k, v in filter(lambda k: (k[0][0] or k[0][1]) and not k[0][2], summary.items()): # == str((False, False, False)), summary.items()):\n",
    "    print(str(k))\n",
    "    print(json.dumps(random.sample(v, k=max(1, len(v))), indent=2))\n",
    "    print(\"-----------------------------------------------\")\n",
    "#{k: v for k, v in summary.items() if k != str((False, False, False))}}, indent=4))\n",
    "print(f\"Lonely baselines: {len(lonely_baselines)}, Lonely exps: {len(lonely_experimental_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Types of lonely baselines: \\n\" + json.dumps([s for s in sorted(lonely_baselines, key = lambda x: x['sanitized_message'])], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps([e for e in experimental_results if \"BUSYBOX/eef\" in e['input_file']], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
